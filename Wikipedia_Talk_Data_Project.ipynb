{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What are the text cleaning methods you tried? What are the ones you have included in the\n",
    "final code?<br>\n",
    "I tried removing all punctuations, removing all numbers, changing text into lowercase, and removing stop words. I included the first three methods: removing all punctuations, removing all numbers, changing text into lowercase in the final code because using these three methods, precision increases; ROC AUC, recall and accuracy remain the same. However, removing stop words decreases ROC AUC, precision, and accuracy, only increases recall. Therefore, I chose not to include it in the final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# remove all punctuations\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# remove all numbers\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\n",
    "\n",
    "# change into lowercase\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove stop words\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = set(stopwords.words('english'))\n",
    "#comments['comment'] = comments['comment'].str.split()\n",
    "#comments['comment'] = comments['comment'].apply(lambda x : [word for word in x if word not in stop])\n",
    "#comments['comment'] = comments['comment'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279                   iraq is not good      usa is bad   \n",
       "2702703       fuck off you little asshole if you want to ...\n",
       "4632658           i have a dick its bigger than yours hahaha\n",
       "6545332       renault   you sad little bpy for driving a ...\n",
       "6545351       renault   you sad little bo for driving a r...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What are the features you considered using? What features did you use in the final code?<br>\n",
    "I considered using word n-gram and character n-gram. When I tried word n-gram, when n increases from 1 to 4, ROC AUC, precision, recall, and accuracy decreases. When I tried character n-gram, when n increases from 1 to 4, ROC AUC, precision, recall, and accuracy increases. The combination of word n-gram and character n-gram performed better than using one of them individually. I used a combination of word unigram and character 4-gram in the final code. It is noticed that the combination of word unigram and character n-gram, when n is larger than 4 (tried 5 and 6), precision, recall, and accuracy start to decrease. The reason might be when the contiguous sequence of character is too long, it becomes closer to word length, leading to weaker performance combining with word unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What optimizations did you add in your code, if any?<br>\n",
    "I don't have additional optimizations in my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# use word unigram and character 4-gram features\n",
    "vectorizerW = TfidfVectorizer(ngram_range = (1,1), analyzer = 'word')\n",
    "vectorizerC = TfidfVectorizer(ngram_range = (1,4), analyzer = 'char')\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('features', combined_features),\n",
    "    ('classifier', LogisticRegression(penalty = 'l2', C = 1.5, solver = 'liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?<br>\n",
    "I tried LogisticRegression(from strawman code), MultinomialNB, RandomForestClassifier, SVC, MLPClassifier, and LinearSVC. I didn't have results from SVC and MLPClassifier since they ran very slow.<br>\n",
    "\n",
    "LogisticRegression:<br>\n",
    "Test ROC AUC: 0.966<br>\n",
    "Test precision: 0.934<br>\n",
    "Test recall: 0.798<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20275   147]<br>\n",
    " [ 1091  1665]]<br>\n",
    "Test accuracy: 0.947<br>\n",
    "\n",
    "MultinomialNB:<br>\n",
    "Test ROC AUC: 0.815<br>\n",
    "Test precision: 0.930<br>\n",
    "Test recall: 0.571<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20407    15]<br>\n",
    " [ 2362   394]]<br>\n",
    "Test accuracy: 0.897<br>\n",
    "\n",
    "RandomForestClassifier:<br>\n",
    "Test ROC AUC: 0.888<br>\n",
    "Test precision: 0.933<br>\n",
    "Test recall: 0.665<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20373    49]<br>\n",
    " [ 1842   914]]<br>\n",
    "Test accuracy: 0.918<br>\n",
    "\n",
    "LinearSVC:<br>\n",
    "Test ROC AUC: 0.962<br>\n",
    "Test precision: 0.926<br>\n",
    "Test recall: 0.817<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20222   200]<br>\n",
    " [  981  1775]]<br>\n",
    "Test accuracy: 0.949<br>\n",
    "\n",
    "The best ML method before tuning hyperparameters is LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What hyperparameters tuning did you do, and by how many percentage points did your\n",
    "accuracy go up?<br>\n",
    "I tried to tune hyperparameters penalty, C, and solver in LogisticRegression. I attempted to tune hyperparameters using GridSearchCV, but GridSearchCV ran too slow to yield results, so I ended up tuning them manually. I tried liblinear, sag, saga, and newton-cg solver, l1 and l2 penalty, and 0.01, 0.1, 1, 1.2, 1.5, 2, 3, 5, 10 C values. The best combination I got was liblinear solver with l2 penalty and 1.5 C value. My accuracy goes up by 0.106 percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#import numpy as np\n",
    "\n",
    "#param_grid = dict(\n",
    "    #classifier__penalty = ['l1', 'l2'],    \n",
    "    #classifier__C = np.logspace(0, 5, 10),\n",
    "    #classifier__solver = ['liblinear', 'sag', 'saga', 'newton-cg']\n",
    "#)\n",
    "#clf = GridSearchCV(clf, param_grid = param_grid, n_jobs = -1)\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "#print(clf.best_score_)\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "    #print(\"%s: %r\" % (param_name, clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.967\n",
      "Test precision: 0.932\n",
      "Test recall: 0.805\n",
      "Test confusion matrix:\n",
      "[[20258   164]\n",
      " [ 1050  1706]]\n",
      "Test accuracy: 0.948\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(trueVals, predictedVals, average = 'macro')\n",
    "print('Test precision: %.3f' %precision)\n",
    "print('Test recall: %.3f' %recall)\n",
    "print('Test confusion matrix:')\n",
    "print(confusion_matrix(trueVals, predictedVals))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(trueVals, predictedVals)\n",
    "print('Test accuracy: %.3f' %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ROC AUC: 0.967<br>\n",
    "Test precision: 0.932<br>\n",
    "Test recall: 0.805<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20258   164]<br>\n",
    " [ 1050  1706]]<br>\n",
    "Test accuracy: 0.948<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. What did you learn from the different metrics? Did you try cross-validation?<br>\n",
    "I learned that the metric AUC ROC is a measure of how good the machine learning model is. Precision is a measure of the correctness of the classifier labeling a sample. Recall is a measure of the ability of the classifier to find all the correct samples that should be found. Confusion matrix is a measure of the accuracy of the classification. Confusion matrix tabulates the number of correct classifications and misclassifications. Accuracy is the fraction of correct predictions made among the total number of predictions made. The combination of these metrics is a powerful indicator of the goodness of the model. I tried cross-validation with k = 5. The results of cross-validation are stable and close to test results. I think 5-fold cross-validation is good enough to validate the stability of the results and can be finished in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. What are your best final Result Metrics? By how much is it better than the strawman figure?\n",
    "Which model gave you this performance?<br>\n",
    "My best final result matrics are:<br>\n",
    "Test ROC AUC: 0.967<br>\n",
    "Test precision: 0.932<br>\n",
    "Test recall: 0.805<br>\n",
    "Test confusion matrix:<br>\n",
    "[[20258   164]<br>\n",
    " [ 1050  1706]]<br>\n",
    "Test accuracy: 0.948<br>\n",
    "Compared to the strawman results, ROC AUC improves from 0.957 to 0.967 (1.04%), precision improves from 0.929 to 0.932 (0.323%), recall improves from 0.772 to 0.805 (4.27%), the misclassifications from confusion matrix changes from 142 and 1236 to 164 and 1050, accuracy improves from 0.941 to 0.948 (0.744%). LogisticRegression model gave me this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation ROC AUC:  [0.962 0.969 0.969 0.969 0.966]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cross_auc = cross_val_score(clf, comments['comment'], comments['attack'], scoring = 'roc_auc', cv = 5)\n",
    "print('Cross validation ROC AUC: ', np.around(cross_auc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation ROC AUC:  [0.962 &nbsp;0.969 &nbsp;0.969 &nbsp;0.969 &nbsp;0.966]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation precision:  [0.918 0.923 0.911 0.921 0.921]\n"
     ]
    }
   ],
   "source": [
    "cross_precision = cross_val_score(clf, comments['comment'], comments['attack'], scoring = 'precision_macro', cv = 5)\n",
    "print('Cross validation precision: ', np.around(cross_precision, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation precision:  [0.918 &nbsp;0.923 &nbsp;0.911 &nbsp;0.921 &nbsp;0.921]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation recall:  [0.806 0.818 0.834 0.815 0.81 ]\n"
     ]
    }
   ],
   "source": [
    "cross_recall = cross_val_score(clf, comments['comment'], comments['attack'], scoring = 'recall_macro', cv = 5)\n",
    "print('Cross validation recall: ', np.around(cross_recall, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation recall:  [0.806 &nbsp;0.818 &nbsp;0.834 &nbsp;0.815 &nbsp;0.81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy:  [0.946 0.949 0.95  0.949 0.947]\n"
     ]
    }
   ],
   "source": [
    "cross_accuracy = cross_val_score(clf, comments['comment'], comments['attack'], scoring = 'accuracy', cv = 5)\n",
    "print('Cross validation accuracy: ', np.around(cross_accuracy, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation accuracy:  [0.946 &nbsp;0.949 &nbsp;0.95 &nbsp;0.949 &nbsp;0.947]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. What is the most interesting thing you learned from doing the report?<br>\n",
    "The most interesting thing I learned is to improve the machine learning model incrementally. For example, I may try to change one variable while keeping other things constant. The order of trying these modifications may make a difference in the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. What was the hardest thing to do?<br>\n",
    "The hardest thing to do is to tune the hyperparameters. Since GridSearchCV is very slow to run, I chose to tune hyperparameters manually and only improved the LogisticRegression model slightly. If I can make use of GridSearchCV or other tools, I may be able to try out a lot more combinations of hyperparameters to achieve better improvement of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
